{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdb85a4",
   "metadata": {},
   "source": [
    "# Modeling Plan:\n",
    "\n",
    "\n",
    "- Three sets of multiple models. \n",
    "- Each model tests these sets:\n",
    "1. Train on original data and test on original data (80/20 split)\n",
    "2. Train on original + synthetic and test on original\n",
    "3. Train on synthetic and test on original\n",
    "\n",
    "\n",
    "\n",
    "## Model Selection\n",
    "- Linear Regression\n",
    "- Random Forests\n",
    "- XGBoost\n",
    "- Hill Climbing? Never done before\n",
    "\n",
    "## Feature Selection\n",
    "- PCA\n",
    "- Regularization (via Lasso?)\n",
    "- Somehow test the collinearity we discovered in the prev notebook, but need to be able to iterate quickly with simpler models? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Plan\n",
    "- to begin, we will just use the original set since it is the smallest. once we refine our approach there, we will create functions so that we can easily (without constant code repitition) call the functions on each of the datasets we split\n",
    "- phase 1 includes:\n",
    "    - split data\n",
    "    - ensure no data leakage (not sure what this really means)\n",
    "    - finalize feature selection?\n",
    "    - scale features? \n",
    "    - define eval metrics (Kaggle uses RMSLE)\n",
    "- phase 2 includes:\n",
    "    - base model (maybe lr and rf first to try to tune params and then test on xgb/hill climbing after)\n",
    "    - cv once we get to rf, xgb, hill climbing\n",
    "    - evaluate on val set\n",
    "    - check for overfitting\n",
    "- phase 3 includes:\n",
    "    - visuals and reporting\n",
    "    - feature importance plots\n",
    "    - analyze residuals?\n",
    "    - documentation\n",
    "    - create final submission (synthetic train on synthetic test) for late kaggle submission out of curiosity \n",
    "\n",
    "\n",
    "### Personal Preferences/Stylizations\n",
    "- when training, i like to see progress bars/output updates every x time interval to ensure that everything is running correctly (quick dopamine when programming == good)\n",
    "- historically, when trying n different models i would just create a standard notebook that repeated the same code n times, just replacing the model definition to iterate -> don't want to do that anymore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161a5a0",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Prep and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea0c0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded synthetic_train dataset: (750000, 9)\n",
      "Loaded og_all dataset: (15000, 9)\n",
      "✅ Combined datasets: 765000 total samples\n",
      "   • Synthetic: 750000 samples\n",
      "   • Original: 15000 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>duration</th>\n",
       "      <th>heart_rate</th>\n",
       "      <th>body_temp</th>\n",
       "      <th>calories</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>male</td>\n",
       "      <td>36</td>\n",
       "      <td>189.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>syn_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>64</td>\n",
       "      <td>163.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>39.7</td>\n",
       "      <td>34.0</td>\n",
       "      <td>syn_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>51</td>\n",
       "      <td>161.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>syn_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>20</td>\n",
       "      <td>192.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>40.7</td>\n",
       "      <td>140.0</td>\n",
       "      <td>syn_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>166.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>40.6</td>\n",
       "      <td>146.0</td>\n",
       "      <td>syn_train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     sex  age  height  weight  duration  heart_rate  body_temp  \\\n",
       "0  0.0    male   36   189.0    82.0      26.0       101.0       41.0   \n",
       "1  1.0  female   64   163.0    60.0       8.0        85.0       39.7   \n",
       "2  2.0  female   51   161.0    64.0       7.0        84.0       39.8   \n",
       "3  3.0    male   20   192.0    90.0      25.0       105.0       40.7   \n",
       "4  4.0  female   38   166.0    61.0      25.0       102.0       40.6   \n",
       "\n",
       "   calories        tag  \n",
       "0     150.0  syn_train  \n",
       "1      34.0  syn_train  \n",
       "2      29.0  syn_train  \n",
       "3     140.0  syn_train  \n",
       "4     146.0  syn_train  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns \n",
    "sys.path.append('../src')\n",
    "\n",
    "import importlib\n",
    "import data_processing\n",
    "importlib.reload(data_processing)\n",
    "from data_processing import *\n",
    "\n",
    "# 1.0 --> Load data\n",
    "synthetic_path = '../data/raw/synthetic_train.csv'  \n",
    "original_path = '../data/raw/og_calories.csv'       \n",
    "combined_df = combine_datasets(synthetic_path, original_path)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2e15eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.1 -> Removing outliers (Pre-Split)\n",
      "==================================================\n",
      "\n",
      "Processing syn_train dataset...\n",
      "Removed 6587 outliers (0.88%) from synthetic dataset\n",
      "syn_train: 750000 -> 743413(6587 removed, 0.88%)\n",
      "\n",
      "Processing og_all dataset...\n",
      "Removed 127 low-end outliers (0.85%) from original dataset\n",
      "og_all: 15000 -> 14873(127 removed, 0.85%)\n",
      "(758286, 11)\n"
     ]
    }
   ],
   "source": [
    "# 1.1 --> Add calorie_burn_rate and perform that outlier Removal (Pre-Splitting)\n",
    "\n",
    "\n",
    "def remove_outliers_by_dataset(combined_df):\n",
    "    '''\n",
    "        Remove outliers applied by dataset type\n",
    "\n",
    "        Args:\n",
    "            combined_df (DataFrame): dataframe with 'tag' column indicating dataset source\n",
    "\n",
    "        Returns:\n",
    "            dataframe with outliers removed\n",
    "    '''\n",
    "    combined_df = calculate_calorie_burn_rate(combined_df)\n",
    "    \n",
    "    print('Step 1.1 -> Removing outliers (Pre-Split)')\n",
    "    print('=' * 50)\n",
    "\n",
    "    clean_df = combined_df.copy()\n",
    "\n",
    "    # track removal statistics\n",
    "    original_counts = {}\n",
    "    clean_counts = {}\n",
    "\n",
    "    # process each dataset type separately \n",
    "    for tag in clean_df['tag'].unique():\n",
    "        print(f'\\nProcessing {tag} dataset...')\n",
    "\n",
    "        # get subset\n",
    "        mask = clean_df['tag'] == tag\n",
    "        subset = clean_df[mask].copy()\n",
    "        original_counts[tag] = len(subset)\n",
    "\n",
    "        # apply appropriate outlier removal\n",
    "        if tag == 'syn_train':\n",
    "            result = remove_calorie_rate_outliers_synthetic(subset)\n",
    "            if isinstance(result, tuple):\n",
    "                subset_clean = result[0] # the function returns df, summary so need to grab first item in tuple\n",
    "        elif tag == 'og_all':\n",
    "            result = remove_calorie_rate_outliers_original(subset, min_rate=2.0)\n",
    "            if isinstance(result, tuple):\n",
    "                subset_clean = result[0]\n",
    "        else:\n",
    "            print(f'Unknown tag: {tag}, skipping outlier removal')\n",
    "            subset_clean = subset\n",
    "\n",
    "        clean_counts[tag] = len(subset_clean)\n",
    "\n",
    "        # update main df\n",
    "        clean_df = clean_df[~mask]\n",
    "        clean_df = pd.concat([clean_df, subset_clean], ignore_index=True)\n",
    "\n",
    "        # progress update\n",
    "        removed = original_counts[tag] - clean_counts[tag]\n",
    "        removal_pct = (removed / original_counts[tag]) * 100\n",
    "        print(f'{tag}: {original_counts[tag]} -> {clean_counts[tag]}'\n",
    "                f'({removed} removed, {removal_pct:.2f}%)')\n",
    "    \n",
    "    return clean_df, original_counts, clean_counts\n",
    "\n",
    "clean_df, original_counts, clean_counts = remove_outliers_by_dataset(combined_df)\n",
    "print(clean_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bebea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.2 --> Splitting Dataset\n",
      "==================================================\n",
      "\n",
      "Split Validation\n",
      "==================================================\n",
      "- og_train: 10411 samples (70.00%)\n",
      "- og_val: 2231 samples (15.00%)\n",
      "- og_test: 2231 samples (15.00%)\n",
      "-synthetic_full: 743413 samples\n",
      "\n",
      "Gender Distribution\n",
      "==================================================\n",
      "-og_train: 49.22% male, 50.78% female\n",
      "-og_val: 49.22% male, 50.78% female\n",
      "-og_test: 49.22% male, 50.78% female\n",
      "-synthetic_full: 49.49% male, 50.51% female\n",
      "\n",
      "Saving Split Datasets\n",
      "==================================================\n",
      "Saved og_train: ../data/processed/og_train.csv\n",
      "Saved og_val: ../data/processed/og_val.csv\n",
      "Saved og_test: ../data/processed/og_test.csv\n",
      "Saved synthetic_full: ../data/processed/synthetic_full.csv\n"
     ]
    }
   ],
   "source": [
    "# 1.2 --> Split Dataset\n",
    "\n",
    "def split_datasets(clean_df, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    '''\n",
    "        Split original dataset into 70/15/15 and keep synthetic data separate due to experiment design\n",
    "\n",
    "        Args:\n",
    "            clean_df (DataFrame): the combined dataframe with outliers removed\n",
    "            test_size (float): proportion of original data used for test set\n",
    "            val_size (float): proportion of original data used for val set\n",
    "            random_state (int): seed for reproducibility\n",
    "\n",
    "        Returns:\n",
    "            dictionary with split datasets\n",
    "    '''\n",
    "\n",
    "    print(f'\\nStep 1.2 --> Splitting Dataset')\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    original_data = clean_df[clean_df['tag'] == 'og_all'].copy()\n",
    "    synthetic_data = clean_df[clean_df['tag'] == 'syn_train'].copy()\n",
    "\n",
    "    # remove tag col for modeling\n",
    "    original_features = original_data.drop(columns=['tag'])\n",
    "    synthetic_features = synthetic_data.drop(columns=['tag'])\n",
    "\n",
    "    # first split (test 15%) and ensuring that the sex split is balanced\n",
    "    train_and_val, test = train_test_split(original_features, test_size=test_size, random_state=random_state, stratify=original_features['sex'])\n",
    "\n",
    "    # second split (val 15% of original)\n",
    "    val_proportion = val_size / (1 - test_size) # adjust for remaining data since we are now taking 15% of 85%\n",
    "    train, val = train_test_split(train_and_val, test_size=val_proportion, random_state=random_state, stratify=train_and_val['sex'])\n",
    "\n",
    "    # create dataset dictionary\n",
    "    datasets = {\n",
    "        'og_train': train,\n",
    "        'og_val': val,\n",
    "        'og_test': test,\n",
    "        'synthetic_full': synthetic_features\n",
    "    }\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "# output and validate splits\n",
    "def validate_splits(datasets):\n",
    "    print(f'\\nSplit Validation')\n",
    "    print('=' * 50)\n",
    "\n",
    "    total_original = sum(len(df) for name, df in datasets.items() if name.startswith('og_'))\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        percentage = (len(df) / total_original) * 100 if name.startswith('og_') else None\n",
    "\n",
    "        if percentage:\n",
    "            print(f'- {name}: {len(df)} samples ({percentage:.2f}%)')\n",
    "        else:\n",
    "            print(f'-{name}: {len(df)} samples')\n",
    "\n",
    "    # verify balanced gender distribution\n",
    "    print(f'\\nGender Distribution')\n",
    "    print('=' * 50)\n",
    "    for name, df in datasets.items():\n",
    "        if 'sex' in df.columns:\n",
    "            gender_dist = df['sex'].value_counts(normalize=True)\n",
    "            male_pct = gender_dist.get('male', 0) * 100\n",
    "            female_pct = gender_dist.get('female', 0) * 100\n",
    "            print(f'-{name}: {male_pct:.2f}% male, {female_pct:.2f}% female')\n",
    "\n",
    "\n",
    "# save the split initial split datasets\n",
    "def save_split_datasets(datasets, output_dir='../data/processed/'):\n",
    "    print(f'\\nSaving Split Datasets')\n",
    "    print('=' * 50)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        file_path = os.path.join(output_dir, f'{name}.csv')\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f'Saved {name}: {file_path}')\n",
    "\n",
    "datasets = split_datasets(clean_df, test_size=.15, val_size=.15, random_state=42)\n",
    "validate_splits(datasets)\n",
    "save_split_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e390bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.3 --> Create Pipeline function to handle Steps 1.1 and 1.2 \n",
    "\n",
    "def run_data_pipeline(combined_df, save_outputs=False):\n",
    "    '''\n",
    "        Run the data prep pipeline\n",
    "\n",
    "        Args:\n",
    "            combined_df (DataFrame): raw combined dataframe \n",
    "            save_outputs (Boolean): determine whether to save the intermediate split datasets\n",
    "        \n",
    "        Returns:\n",
    "            dictionary with clean split datasets\n",
    "    '''\n",
    "\n",
    "    print(f'\\nStarting Data Preparation Pipeline')\n",
    "    print('=' * 50)\n",
    "\n",
    "    # 1.1\n",
    "    clean_df, original_counts, clean_counts = remove_outliers_by_dataset(combined_df)\n",
    "    \n",
    "\n",
    "    # 1.2\n",
    "    datasets = split_datasets(clean_df)\n",
    "    validate_splits(datasets)\n",
    "\n",
    "    if save_outputs:\n",
    "        save_split_datasets(datasets)\n",
    "\n",
    "    print(f'Data Prep Complete')\n",
    "\n",
    "    return datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
